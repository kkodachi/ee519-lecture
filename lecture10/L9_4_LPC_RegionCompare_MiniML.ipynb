{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d70992a",
   "metadata": {},
   "source": [
    "# EE519 ‚Äî Lecture 9 (Linear Prediction / LPC) ‚Äî Notebook 9.4\n",
    "## Where LPC works/fails + optional mini ML demo\n",
    "\n",
    "**Theme:** Compare vowel vs fricative vs silence; optionally train a tiny classifier.\n",
    "\n",
    "---\n",
    "### üß≠ In-class workflow\n",
    "1. Read the short explanation above each code cell\n",
    "2. Predict what you expect to see\n",
    "3. Run\n",
    "4. Save at least one key figure\n",
    "\n",
    "### üßØ Debugging quick panel (‚ÄúIf you see X, do Y‚Äù)\n",
    "- **Module import error** ‚Üí run the ‚ÄúEnvironment & imports‚Äù cell again; restart kernel if needed.\n",
    "- **Audio playback is silent** ‚Üí re-record closer to mic; ensure waveform peak is not near zero.\n",
    "- **`frame_selections` missing** ‚Üí go back to Notebook 9.0 and define time ranges / frames, then save to manifest.\n",
    "- **LPC envelope looks too wiggly** ‚Üí reduce order `p` (try 10‚Äì16).\n",
    "- **LPC envelope looks too flat** ‚Üí increase order `p` slightly or pick a steadier vowel region.\n",
    "- **FFT vs LPC don‚Äôt ‚Äúoverlay‚Äù** ‚Üí use the provided ‚Äúnormalize-to-peak‚Äù plot (shape comparison) cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a2b48",
   "metadata": {},
   "source": [
    "### üéØ Learning goals\n",
    "- Run end-to-end without manual clip/frame prompts (assumes Notebook 9.0 selections saved)\n",
    "- Save key plots to the project folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1203975",
   "metadata": {},
   "source": [
    "## 0. Environment & imports (run this first)\n",
    "\n",
    "This notebook uses:\n",
    "- `numpy`, `matplotlib`\n",
    "- `scipy` (signal + linalg)\n",
    "- optional: `sounddevice` (recording)\n",
    "- optional: `sklearn` (mini ML demo only)\n",
    "\n",
    "If any import fails, the cell prints what to do next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d437f55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy imports: ‚úÖ\n",
      "sounddevice: ‚úÖ (recording enabled)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Core scipy imports (required)\n",
    "try:\n",
    "    import scipy.signal as sig\n",
    "    import scipy.linalg as la\n",
    "    import scipy.io.wavfile as wavfile\n",
    "    SCIPY_OK = True\n",
    "    print(\"scipy imports: ‚úÖ\")\n",
    "except Exception as e:\n",
    "    SCIPY_OK = False\n",
    "    print(\"scipy imports: ‚ùå\")\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "# Optional recording\n",
    "try:\n",
    "    import sounddevice as sd\n",
    "    HAS_SD = True\n",
    "    print(\"sounddevice: ‚úÖ (recording enabled)\")\n",
    "except Exception as e:\n",
    "    HAS_SD = False\n",
    "    print(\"sounddevice: ‚ùå (recording disabled)\")\n",
    "\n",
    "from pathlib import Path\n",
    "import json, os, time\n",
    "from IPython.display import Audio, display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6492b",
   "metadata": {},
   "source": [
    "## 1. Project + manifest workflow (same spirit as Lectures 7/8)\n",
    "\n",
    "We will use one project folder:\n",
    "```\n",
    "EE519_L9_Project/\n",
    "  recordings/\n",
    "  figures/\n",
    "  features/\n",
    "  cache/\n",
    "  manifest.json\n",
    "```\n",
    "\n",
    "‚úÖ You can re-run this cell any time safely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2ef388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest clips: 9\n",
      "Project dir: C:\\Users\\K\\Documents\\usc\\ee519\\ee519-lecture\\lecture10\\EE519_L9_Project\n"
     ]
    }
   ],
   "source": [
    "PROJECT_DIR = Path(\"EE519_L9_Project\")\n",
    "REC_DIR = PROJECT_DIR / \"recordings\"\n",
    "FIG_DIR = PROJECT_DIR / \"figures\"\n",
    "FEAT_DIR = PROJECT_DIR / \"features\"\n",
    "CACHE_DIR = PROJECT_DIR / \"cache\"\n",
    "\n",
    "for d in [PROJECT_DIR, REC_DIR, FIG_DIR, FEAT_DIR, CACHE_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MANIFEST_PATH = PROJECT_DIR / \"manifest.json\"\n",
    "\n",
    "def load_manifest():\n",
    "    if MANIFEST_PATH.exists():\n",
    "        return json.loads(MANIFEST_PATH.read_text())\n",
    "    return {\"clips\": [], \"meta\": {\"created\": time.time(), \"course\":\"EE519\", \"lecture\":9}}\n",
    "\n",
    "def save_manifest(m):\n",
    "    MANIFEST_PATH.write_text(json.dumps(m, indent=2))\n",
    "\n",
    "manifest = load_manifest()\n",
    "print(\"Manifest clips:\", len(manifest[\"clips\"]))\n",
    "print(\"Project dir:\", PROJECT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61828f3",
   "metadata": {},
   "source": [
    "## 2. Utilities (audio I/O, framing, STFT, saving figures)\n",
    "\n",
    "These helpers are used throughout Lecture 9 notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f8f54ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wav(path):\n",
    "    fs, x = wavfile.read(path)\n",
    "    x = x.astype(np.float32)\n",
    "    if x.ndim > 1:\n",
    "        x = x.mean(axis=1)\n",
    "    if np.max(np.abs(x)) > 1.5:\n",
    "        x = x / 32768.0\n",
    "    return fs, x\n",
    "\n",
    "def peak_normalize(x, target=0.95):\n",
    "    m = np.max(np.abs(x)) + 1e-12\n",
    "    return x * (target / m)\n",
    "\n",
    "def play_audio(x, fs, label=\"\"):\n",
    "    print(label, f\"(fs={fs}, length={len(x)/fs:.2f}s)\")\n",
    "    display(Audio(x, rate=fs))\n",
    "\n",
    "def savefig(name):\n",
    "    out = FIG_DIR / name\n",
    "    plt.savefig(out, dpi=180, bbox_inches=\"tight\")\n",
    "    print(\"Saved:\", out)\n",
    "\n",
    "def hann(N):\n",
    "    return np.hanning(N).astype(np.float32)\n",
    "\n",
    "def frame_signal(x, N, H):\n",
    "    if len(x) < N:\n",
    "        raise ValueError(\"Signal shorter than frame length N.\")\n",
    "    num = 1 + (len(x) - N) // H\n",
    "    frames = np.stack([x[i*H:i*H+N] for i in range(num)], axis=0)\n",
    "    return frames\n",
    "\n",
    "def stft_scipy(x, fs, win_ms=25, hop_ms=10, nfft=None, window=\"hann\"):\n",
    "    N = int(win_ms * 1e-3 * fs)\n",
    "    H = int(hop_ms * 1e-3 * fs)\n",
    "    if nfft is None:\n",
    "        nfft = 1 << int(np.ceil(np.log2(N)))\n",
    "    f, t, Z = sig.stft(x, fs=fs, window=window, nperseg=N, noverlap=N-H, nfft=nfft, boundary=None, padded=False)\n",
    "    return f, t, Z, N, H\n",
    "\n",
    "def plot_spectrogram(Z, fs, title, fmax=8000):\n",
    "    S = 20*np.log10(np.abs(Z)+1e-12)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.imshow(S, origin=\"lower\", aspect=\"auto\",\n",
    "               extent=[0, Z.shape[1], 0, fs/2])\n",
    "    plt.ylim([0, fmax])\n",
    "    plt.colorbar(label=\"dB\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Frame index\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc32dc",
   "metadata": {},
   "source": [
    "## LPC core functions (used in Notebooks 9.1‚Äì9.4)\n",
    "\n",
    "### Important fix vs earlier versions\n",
    "- `toeplitz` is in `scipy.linalg`, not `scipy.signal`.\n",
    "- We therefore use `la.toeplitz` to avoid errors.\n",
    "\n",
    "### Autocorrelation convention\n",
    "We use a **biased** autocorrelation estimate:\n",
    "\\$\n",
    "r[k] = \\sum_{n=0}^{N-1-k} x[n]\\,x[n+k]\n",
    "\\$\n",
    "\n",
    "This is common in LPC autocorrelation method demonstrations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b46e882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorr_biased(x, p):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    r = np.zeros(p+1, dtype=np.float64)\n",
    "    for k in range(p+1):\n",
    "        r[k] = np.sum(x[:len(x)-k] * x[k:])\n",
    "    return r\n",
    "\n",
    "def lpc_autocorr_method(x, p):\n",
    "    r = autocorr_biased(x, p)\n",
    "    R = la.toeplitz(r[:-1])  # r[0..p-1]\n",
    "    rhs = -r[1:]\n",
    "    a = np.linalg.solve(R + 1e-12*np.eye(p), rhs)\n",
    "    return a, r\n",
    "\n",
    "def lpc_residual(x, a):\n",
    "    A = np.concatenate([[1.0], a])\n",
    "    e = sig.lfilter(A, [1.0], x)\n",
    "    return e\n",
    "\n",
    "def lpc_envelope_db(a, fs, nfft=4096):\n",
    "    A = np.concatenate([[1.0], a])\n",
    "    # Use freqz (stable, consistent)\n",
    "    w, h = sig.freqz([1.0], A, worN=nfft, fs=fs)\n",
    "    env_db = 20*np.log10(np.abs(h)+1e-12)\n",
    "    return w, env_db\n",
    "\n",
    "def fft_mag_db(x, fs, nfft=4096):\n",
    "    X = np.fft.rfft(x, n=nfft)\n",
    "    f = np.fft.rfftfreq(nfft, 1/fs)\n",
    "    mag_db = 20*np.log10(np.abs(X)+1e-12)\n",
    "    return f, mag_db\n",
    "\n",
    "def normalize_to_peak(y_db):\n",
    "    return y_db - np.max(y_db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d78557",
   "metadata": {},
   "source": [
    "## Load a clip that already has `frame_selections`\n",
    "\n",
    "‚úÖ If this errors, go back to **Notebook 9.0**, select time ranges, and save to manifest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2674a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using clip: 6 F01_fric_s.wav | label: fricative\n",
      "Counts | vowel: 30 fric: 10 sil: 4\n"
     ]
    }
   ],
   "source": [
    "def pick_first_clip_with_selections(prefer_label=\"vowel\"):\n",
    "    m = load_manifest()\n",
    "    # first try preferred label\n",
    "    for i,c in enumerate(m[\"clips\"]):\n",
    "        if c.get(\"label\")==prefer_label and \"frame_selections\" in c and len(c[\"frame_selections\"].get(\"vowel_frames\",[]))>0:\n",
    "            return i,c,m\n",
    "    # otherwise any with selections\n",
    "    for i,c in enumerate(m[\"clips\"]):\n",
    "        if \"frame_selections\" in c:\n",
    "            return i,c,m\n",
    "    raise RuntimeError(\"No clip has frame_selections. Run Notebook 9.0 to select and save frames.\")\n",
    "\n",
    "CLIP_IDX, clip, manifest = pick_first_clip_with_selections(\"vowel\")\n",
    "print(\"Using clip:\", CLIP_IDX, clip[\"filename\"], \"| label:\", clip.get(\"label\"))\n",
    "\n",
    "fs, x = read_wav(REC_DIR / clip[\"filename\"])\n",
    "x = peak_normalize(x)\n",
    "\n",
    "sel = clip[\"frame_selections\"]\n",
    "WIN_MS = sel.get(\"win_ms\", 25)\n",
    "HOP_MS = sel.get(\"hop_ms\", 10)\n",
    "N = int(WIN_MS*1e-3*fs)\n",
    "H = int(HOP_MS*1e-3*fs)\n",
    "\n",
    "frames = frame_signal(x, N, H) * hann(N)[None,:]\n",
    "vowel_frames = sel.get(\"vowel_frames\", [])\n",
    "fric_frames = sel.get(\"fricative_frames\", [])\n",
    "sil_frames = sel.get(\"silence_frames\", [])\n",
    "\n",
    "print(\"Counts | vowel:\", len(vowel_frames), \"fric:\", len(fric_frames), \"sil:\", len(sil_frames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007db78a",
   "metadata": {},
   "source": [
    "### üîß Practical audio conditioning (recommended)\n",
    "Two small steps often make LPC envelopes cleaner:\n",
    "\n",
    "1) **DC removal**: removes microphone offset  \n",
    "2) **Pre-emphasis**: boosts high frequencies so the envelope is more balanced\n",
    "\n",
    "You can toggle this on/off to see the effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a974d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied DC removal + pre-emphasis.\n"
     ]
    }
   ],
   "source": [
    "USE_PREEMPH = True\n",
    "PREEMPH_ALPHA = 0.97\n",
    "\n",
    "if USE_PREEMPH:\n",
    "    x = x - np.mean(x)\n",
    "    x = sig.lfilter([1, -PREEMPH_ALPHA], [1], x)\n",
    "    x = peak_normalize(x)\n",
    "    print(\"Applied DC removal + pre-emphasis.\")\n",
    "else:\n",
    "    print(\"Skipped pre-emphasis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9b385f",
   "metadata": {},
   "source": [
    "## Mini ML demo (optional): vowel vs fricative classification\n",
    "\n",
    "We build a tiny dataset from your selected frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf63af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn: ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "    SKLEARN_OK = True\n",
    "    print(\"sklearn: ‚úÖ\")\n",
    "except Exception as e:\n",
    "    SKLEARN_OK = False\n",
    "    print(\"sklearn: ‚ùå (skip this section)\")\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ebceca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: (40, 13) labels: (40,)\n",
      "Accuracy: 0.75\n",
      "Confusion matrix:\n",
      " [[ 0  9]\n",
      " [ 0 27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         9\n",
      "           1       0.75      1.00      0.86        27\n",
      "\n",
      "    accuracy                           0.75        36\n",
      "   macro avg       0.38      0.50      0.43        36\n",
      "weighted avg       0.56      0.75      0.64        36\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\K\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "if SKLEARN_OK:\n",
    "    def lpc_feature(xf, p=12):\n",
    "        a,_ = lpc_autocorr_method(xf, p)\n",
    "        e = lpc_residual(xf, a)\n",
    "        feat = np.concatenate([a, [np.log(np.mean(e**2)+1e-12)]])\n",
    "        return feat\n",
    "\n",
    "    X_feat = []\n",
    "    y = []\n",
    "    for idx in vowel_frames:\n",
    "        X_feat.append(lpc_feature(frames[idx], p=12)); y.append(1)\n",
    "    for idx in fric_frames:\n",
    "        X_feat.append(lpc_feature(frames[idx], p=12)); y.append(0)\n",
    "\n",
    "    X_feat = np.stack(X_feat, axis=0)\n",
    "    y = np.array(y)\n",
    "    print(\"Dataset:\", X_feat.shape, \"labels:\", y.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_feat, y, test_size=0.9, random_state=0, stratify=y)\n",
    "    clf = LogisticRegression(max_iter=3000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, pred))\n",
    "    print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a406b75",
   "metadata": {},
   "source": [
    "### üéß Listen to the residual (very important intuition)\n",
    "- For a **vowel**, residual should sound like a buzzy excitation (pitch pulses).\n",
    "- For a **fricative**, residual often resembles noise (because the model is mismatched).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e156f8",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ What you learned (Notebook 9.4)\n",
    "- You ran the LPC pipeline without fighting imports/toeplitz errors.\n",
    "- You compared FFT vs LPC envelope using a **normalized-to-peak** plot (shape match).\n",
    "- You saved figures into the project folder for later slides/reports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db1b3e",
   "metadata": {},
   "source": [
    "---\n",
    "## üß† Reflection (Notebook 9.4)\n",
    "\n",
    "### What you learned\n",
    "- When LPC works well (steady voiced segments) and when it fails (fricatives/silence).\n",
    "- How the LPC residual and envelope behave across vowel vs fricative vs silence.\n",
    "- How LPC-based features can support a tiny classifier (concept demo).\n",
    "\n",
    "### Common mistakes to notice (and fix next time)\n",
    "- Expecting an all-pole model to fit fricative spectra (noise-like + zeros).\n",
    "- Training a classifier on too few samples and over-interpreting accuracy.\n",
    "- Mixing labels because frame selections included multiple phonetic events.\n",
    "\n",
    "### Reflective questions\n",
    "1. Describe one clear visual difference between vowel and fricative LPC envelopes.\n",
    "2. Why does an all-pole model struggle on fricatives (conceptually)?\n",
    "3. If you wanted to classify more reliably, what data/feature improvements would you make?\n",
    "4. What is one takeaway rule you will remember about using LPC in practice?\n",
    "\n",
    "### Quick self-check\n",
    "- [ ] I can state one scenario where LPC is appropriate and one where it is not.\n",
    "- [ ] I can explain why tiny ML demos are useful but not definitive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d1a22",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "1. Vowel LPC envelopes are smoother with peaks from the formants while fricatives are flatter.\n",
    "2. Fricatives aren't periodic so they aren't as easily represented. This can cause an all-pole model to fail.\n",
    "3. We could use different frames or other features such as cepstral. I will be using MFCC for my project.\n",
    "4. Not to pick too high order but just enought to capture formants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a14d94",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb2851-afc3-4cc4-8377-a8b6988e0f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
