{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acba0154",
   "metadata": {},
   "source": [
    "# EE 519 ‚Äî Frequency-domain Analysis of Speech (Lectures 7-8)\n",
    "## Notebook ‚Äî Manual STFT + Inspect Specific Frames (by time)\n",
    "\n",
    "---\n",
    "## What you will do\n",
    "1. Load a saved speech clip from your manifest.\n",
    "2. Compute STFT (manual).\n",
    "3. Plot spectrogram and **pick times of interest**.\n",
    "4. Convert times ‚Üí frame indices and plot **frame spectra**.\n",
    "5. Save plots for your lab report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c13145",
   "metadata": {},
   "source": [
    "### How to use Lecture 7 notebooks in-class (hands-on)\n",
    "**Goal:** build intuition for STFT by repeatedly analyzing *your own* recordings under different window/hop choices.\n",
    "\n",
    "**Workflow for the whole lecture**\n",
    "1. Record multiple clips once (Notebook 7.0) and they are saved + registered in a manifest.\n",
    "2. Every later notebook reuses the same recordings (no re-recording needed).\n",
    "3. Always pick **meaningful time regions** (vowel middle, fricative middle, stop burst, silence) ‚Äî not just the first frames.\n",
    "\n",
    "**Where things are saved**\n",
    "- Audio: `EE519_L7_Project/audio/`\n",
    "- Figures: `EE519_L7_Project/figures/`\n",
    "- Manifest: `EE519_L7_Project/manifest.json`\n",
    "\n",
    "**Pro tip:** keep filenames consistent across the class:  \n",
    "`vowel_a_soft`, `vowel_a_loud`, `sentence_fast`, `sentence_slow`, `fricative_s`, `stop_pa`, `silence_room`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0807c475",
   "metadata": {},
   "source": [
    "### ‚úÖ Clip checklist (quick self-check)\n",
    "By the end of Notebook you should have **at least 8 clips** registered in your manifest:\n",
    "\n",
    "**Required categories**\n",
    "- **Vowel**: one steady vowel (e.g., /a/ or /i/) ‚Äî *soft* and *loud*\n",
    "- **Sentence**: one short sentence ‚Äî *slow* and *fast*\n",
    "- **Fricative**: sustained /s/ or /sh/\n",
    "- **Stop**: repeated /pa pa pa/ or /ta ta ta/ (captures bursts + closures)\n",
    "- **Silence**: 2‚Äì3 seconds of room silence\n",
    "\n",
    "If you're missing any, record them now in 7.0 (it takes 2-3 minutes and makes later analysis much clearer).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab353a4",
   "metadata": {},
   "source": [
    "## 0) Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5612e42",
   "metadata": {},
   "source": [
    "#### üéôÔ∏è Before you record: tips for clean data\n",
    "- Sit ~15‚Äì25 cm from the mic.\n",
    "- Avoid tapping the laptop/desk (low-frequency thumps).\n",
    "- Record at least **2‚Äì3 seconds** per clip.\n",
    "- For vowel: hold a steady pitch (don‚Äôt glide).\n",
    "- For sentence: keep content the same across slow/fast versions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82af8d36",
   "metadata": {},
   "source": [
    "#### üíæ Save your artifacts\n",
    "Saving plots helps you build a personal ‚Äúspeech atlas‚Äù you can review before exams/projects.\n",
    "If your saved figures folder is empty, check:\n",
    "- you ran the plotting cell\n",
    "- the notebook has write permission in the current folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68804cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.signal import get_window, chirp\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 3)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "# ---------------------------\n",
    "# Project folders (auto-create)\n",
    "# ---------------------------\n",
    "PROJECT_DIR = Path(\"EE519_L7_Project\")\n",
    "AUDIO_DIR   = PROJECT_DIR / \"audio\"\n",
    "FIG_DIR     = PROJECT_DIR / \"figures\"\n",
    "RESULTS_DIR = PROJECT_DIR / \"results\"\n",
    "for d in [PROJECT_DIR, AUDIO_DIR, FIG_DIR, RESULTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MANIFEST_PATH = PROJECT_DIR / \"manifest.json\"\n",
    "\n",
    "def _to_float_mono(y):\n",
    "    \"\"\"Convert audio array to float32 mono in [-1, 1].\"\"\"\n",
    "    y = np.asarray(y)\n",
    "    if y.ndim == 2:\n",
    "        y = y.mean(axis=1)\n",
    "    if np.issubdtype(y.dtype, np.integer):\n",
    "        y = y.astype(np.float32) / (np.iinfo(y.dtype).max + 1e-12)\n",
    "    else:\n",
    "        y = y.astype(np.float32)\n",
    "    mx = np.max(np.abs(y)) + 1e-12\n",
    "    if mx > 1.0:\n",
    "        y = y / mx\n",
    "    return y\n",
    "\n",
    "def load_wav(path):\n",
    "    fs, y = wavfile.read(path)\n",
    "    return fs, _to_float_mono(y)\n",
    "\n",
    "def save_wav(path, fs, y):\n",
    "    \"\"\"Save float audio in [-1,1] to 16-bit PCM WAV.\"\"\"\n",
    "    y16 = np.clip(y, -1.0, 1.0)\n",
    "    y16 = (y16 * 32767).astype(np.int16)\n",
    "    wavfile.write(str(path), fs, y16)\n",
    "\n",
    "def play_audio(y, fs, label=None):\n",
    "    if label:\n",
    "        print(label)\n",
    "    display(Audio(y, rate=fs))\n",
    "\n",
    "def plot_waveform(y, fs, title=\"Waveform\", tlim=None, save_as=None):\n",
    "    t = np.arange(len(y)) / fs\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(t, y, linewidth=1)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(title)\n",
    "    if tlim is not None:\n",
    "        plt.xlim(tlim)\n",
    "    if save_as is not None:\n",
    "        out = FIG_DIR / save_as\n",
    "        plt.savefig(out, dpi=160, bbox_inches=\"tight\")\n",
    "        print(\"Saved figure ->\", out)\n",
    "    plt.show()\n",
    "\n",
    "def mag_spectrum(y, fs, nfft=None, db=True):\n",
    "    y = np.asarray(y, dtype=np.float32)\n",
    "    if nfft is None:\n",
    "        nfft = int(2**np.ceil(np.log2(len(y))))\n",
    "    Y = fft(y, n=nfft)\n",
    "    f = fftfreq(nfft, d=1/fs)\n",
    "    idx = f >= 0\n",
    "    mag = np.abs(Y[idx])\n",
    "    if db:\n",
    "        mag = 20*np.log10(mag + 1e-10)\n",
    "    return f[idx], mag\n",
    "\n",
    "def plot_spectrum(y, fs, title=\"Magnitude Spectrum\", fmax=None, nfft=None, db=True, save_as=None):\n",
    "    f, mag = mag_spectrum(y, fs, nfft=nfft, db=db)\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(f, mag, linewidth=1)\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Magnitude (dB)\" if db else \"Magnitude\")\n",
    "    plt.title(title)\n",
    "    if fmax is not None:\n",
    "        plt.xlim(0, fmax)\n",
    "    if save_as is not None:\n",
    "        out = FIG_DIR / save_as\n",
    "        plt.savefig(out, dpi=160, bbox_inches=\"tight\")\n",
    "        print(\"Saved figure ->\", out)\n",
    "    plt.show()\n",
    "\n",
    "def rms(y):\n",
    "    y = np.asarray(y, dtype=np.float32)\n",
    "    return float(np.sqrt(np.mean(y**2) + 1e-12))\n",
    "\n",
    "def add_white_noise(y, snr_db, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y = np.asarray(y, dtype=np.float32)\n",
    "    sig_pwr = np.mean(y**2) + 1e-12\n",
    "    noise_pwr = sig_pwr / (10**(snr_db/10))\n",
    "    noise = rng.standard_normal(len(y), dtype=np.float32) * np.sqrt(noise_pwr)\n",
    "    return y + noise\n",
    "\n",
    "def select_segment(y, fs, start_s, end_s):\n",
    "    s = int(start_s*fs); e = int(end_s*fs)\n",
    "    s = max(0, min(s, len(y)))\n",
    "    e = max(0, min(e, len(y)))\n",
    "    if e <= s:\n",
    "        raise ValueError(\"end_s must be > start_s and within signal duration.\")\n",
    "    return y[s:e]\n",
    "\n",
    "# ---------------------------\n",
    "# Recording utilities\n",
    "# ---------------------------\n",
    "SOUNDDEVICE_OK = False\n",
    "try:\n",
    "    import sounddevice as sd\n",
    "    SOUNDDEVICE_OK = True\n",
    "except Exception:\n",
    "    SOUNDDEVICE_OK = False\n",
    "\n",
    "def record_audio(seconds=3.0, fs=16000, channels=1):\n",
    "    if not SOUNDDEVICE_OK:\n",
    "        raise RuntimeError(\"sounddevice not available. Upload WAVs or install sounddevice locally.\")\n",
    "    print(f\"Recording {seconds:.1f}s at {fs} Hz... (speak now)\")\n",
    "    y = sd.rec(int(seconds*fs), samplerate=fs, channels=channels, dtype='float32')\n",
    "    sd.wait()\n",
    "    y = _to_float_mono(y)\n",
    "    print(\"Done. RMS:\", rms(y))\n",
    "    return fs, y\n",
    "\n",
    "def load_manifest():\n",
    "    if MANIFEST_PATH.exists():\n",
    "        return json.loads(MANIFEST_PATH.read_text(encoding=\"utf-8\"))\n",
    "    return {\"audio_clips\": [], \"notes\": []}\n",
    "\n",
    "def save_manifest(m):\n",
    "    MANIFEST_PATH.write_text(json.dumps(m, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def register_clip(name, path, fs, duration_s, tags=None, notes=\"\"):\n",
    "    m = load_manifest()\n",
    "    m[\"audio_clips\"].append({\n",
    "        \"name\": name,\n",
    "        \"path\": str(path),\n",
    "        \"fs\": fs,\n",
    "        \"duration_s\": float(duration_s),\n",
    "        \"tags\": tags or [],\n",
    "        \"notes\": notes\n",
    "    })\n",
    "    save_manifest(m)\n",
    "\n",
    "def list_clips():\n",
    "    m = load_manifest()\n",
    "    if len(m[\"audio_clips\"]) == 0:\n",
    "        print(\"No clips registered yet.\")\n",
    "        return\n",
    "    for i, c in enumerate(m[\"audio_clips\"]):\n",
    "        print(f\"[{i}] {c['name']} | {c['duration_s']:.2f}s | fs={c['fs']} | tags={c['tags']} | file={c['path']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709d372",
   "metadata": {},
   "source": [
    "#### üîß Optional automated check (run anytime)\n",
    "This cell reads your `manifest.json` and tells you what clip types you might be missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec00e28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest clips: 2\n",
      "‚ö†Ô∏è Missing (recommended):\n",
      " - vowel (soft)\n",
      " - vowel (loud)\n",
      " - sentence (slow)\n",
      " - sentence (fast)\n",
      " - fricative\n",
      " - stop/burst\n",
      " - silence\n"
     ]
    }
   ],
   "source": [
    "# Automated checklist (uses tags and/or names)\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "MANIFEST_PATH = Path(\"EE519_L7_Project/manifest.json\")\n",
    "if not MANIFEST_PATH.exists():\n",
    "    print(\"No manifest found yet. Run Notebook 7.0 and record at least one clip.\")\n",
    "else:\n",
    "    m = json.loads(MANIFEST_PATH.read_text(encoding=\"utf-8\"))\n",
    "    clips = m.get(\"audio_clips\", [])\n",
    "    names = [c.get(\"name\",\"\").lower() for c in clips]\n",
    "    tags  = [set([t.lower() for t in c.get(\"tags\",[])]) for c in clips]\n",
    "\n",
    "    def has(pattern):\n",
    "        rgx = re.compile(pattern)\n",
    "        return any(rgx.search(n) for n in names)\n",
    "\n",
    "    def has_tag(t):\n",
    "        return any(t in tg for tg in tags)\n",
    "\n",
    "    req = {\n",
    "        \"vowel (soft)\": has(\"vowel\") and (has(\"soft\") or has_tag(\"soft\")),\n",
    "        \"vowel (loud)\": has(\"vowel\") and (has(\"loud\") or has_tag(\"loud\")),\n",
    "        \"sentence (slow)\": has(\"sentence\") and (has(\"slow\") or has_tag(\"slow\")),\n",
    "        \"sentence (fast)\": has(\"sentence\") and (has(\"fast\") or has_tag(\"fast\")),\n",
    "        \"fricative\": has(\"fric\") or has(\"s_\") or has_tag(\"fricative\"),\n",
    "        \"stop/burst\": has(\"stop\") or has(\"pa\") or has(\"ta\") or has_tag(\"stop\"),\n",
    "        \"silence\": has(\"silence\") or has_tag(\"silence\"),\n",
    "    }\n",
    "\n",
    "    print(\"Manifest clips:\", len(clips))\n",
    "    missing = [k for k,v in req.items() if not v]\n",
    "    if not missing:\n",
    "        print(\"‚úÖ Looks good ‚Äî you have the recommended set.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Missing (recommended):\")\n",
    "        for k in missing:\n",
    "            print(\" -\", k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c7d653",
   "metadata": {},
   "source": [
    "#### üîç Before you run: what will change when you change window/hop?\n",
    "- **Short window** ‚Üí better **time** resolution, blurrier frequency (wideband look)\n",
    "- **Long window** ‚Üí better **frequency** resolution, blurrier time (narrowband look)\n",
    "- **Smaller hop** ‚Üí more overlap ‚Üí smoother time evolution, more computation\n",
    "\n",
    "**Common issues**\n",
    "- If harmonics look ‚Äúfuzzy‚Äù: increase window length (for narrowband).\n",
    "- If stop bursts look ‚Äúsmeared‚Äù: shorten window (for wideband).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e20506e",
   "metadata": {},
   "source": [
    "#### üéØ Action: pick *meaningful* time points (not sequential frames)\n",
    "Use the waveform/spectrogram to pick times from **different phenomena**:\n",
    "- vowel middle\n",
    "- fricative middle\n",
    "- stop burst peak\n",
    "- silence/closure\n",
    "\n",
    "**Why?** You want to compare spectra across *sound types*, not across adjacent time slices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0807be6",
   "metadata": {},
   "source": [
    "#### üíæ Save your artifacts\n",
    "Saving plots helps you build a personal ‚Äúspeech atlas‚Äù you can review before exams/projects.\n",
    "If your saved figures folder is empty, check:\n",
    "- you ran the plotting cell\n",
    "- the notebook has write permission in the current folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03cf259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft_manual(y, fs, win_length_ms=25, hop_ms=10, window=\"hann\", nfft=None, center=False):\n",
    "    \"\"\"Manual STFT (analysis only). Returns f (Hz), t (s), X (frames x freqs), meta.\"\"\"\n",
    "    y = np.asarray(y, dtype=np.float32)\n",
    "    N = int(round(win_length_ms * 1e-3 * fs))\n",
    "    H = int(round(hop_ms * 1e-3 * fs))\n",
    "    if N <= 0 or H <= 0:\n",
    "        raise ValueError(\"Window length and hop must be positive.\")\n",
    "    if nfft is None:\n",
    "        nfft = int(2**np.ceil(np.log2(N)))\n",
    "    if nfft < N:\n",
    "        raise ValueError(\"nfft must be >= N.\")\n",
    "    w = get_window(window, N, fftbins=True).astype(np.float32)\n",
    "    if center:\n",
    "        pad = N // 2\n",
    "        y = np.pad(y, (pad, pad), mode=\"constant\")\n",
    "    num_frames = 1 + (len(y) - N) // H\n",
    "    if num_frames <= 0:\n",
    "        raise ValueError(\"Signal too short for chosen window length.\")\n",
    "    X = np.empty((num_frames, nfft//2 + 1), dtype=np.complex64)\n",
    "    for m in range(num_frames):\n",
    "        start = m * H\n",
    "        frame = y[start:start+N] * w\n",
    "        X[m, :] = np.fft.rfft(frame, n=nfft)\n",
    "    f = np.fft.rfftfreq(nfft, d=1/fs)\n",
    "    t = (np.arange(num_frames) * H) / fs\n",
    "    meta = {\"N\": N, \"H\": H, \"nfft\": nfft, \"window\": window, \"center\": center,\n",
    "            \"win_length_ms\": win_length_ms, \"hop_ms\": hop_ms}\n",
    "    return f, t, X, meta\n",
    "\n",
    "def spec_db(X):\n",
    "    return 20*np.log10(np.abs(X) + 1e-10)\n",
    "\n",
    "def plot_spectrogram_db(f, t, X, title=\"Spectrogram (dB)\", fmax=None, vmin=None, vmax=None, save_as=None):\n",
    "    S = spec_db(X)  # frames x freqs\n",
    "    plt.figure(figsize=(12, 4.5))\n",
    "    plt.imshow(S.T, origin=\"lower\", aspect=\"auto\",\n",
    "               extent=[t[0], t[-1], f[0], f[-1]],\n",
    "               vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar(label=\"dB\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.title(title)\n",
    "    if fmax is not None:\n",
    "        plt.ylim(0, fmax)\n",
    "    if save_as is not None:\n",
    "        out = FIG_DIR / save_as\n",
    "        plt.savefig(out, dpi=160, bbox_inches=\"tight\")\n",
    "        print(\"Saved figure ->\", out)\n",
    "    plt.show()\n",
    "\n",
    "def frame_index_from_time(t_sec, fs, hop_ms):\n",
    "    H = int(round(hop_ms * 1e-3 * fs))\n",
    "    return int(round(t_sec * fs / H))\n",
    "\n",
    "def plot_frame_spectrum_from_stft(f, X, frame_idx, title=\"\", fmax=None, save_as=None):\n",
    "    mag_db = 20*np.log10(np.abs(X[frame_idx]) + 1e-10)\n",
    "    plt.figure(figsize=(12,3))\n",
    "    plt.plot(f, mag_db, linewidth=1)\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Magnitude (dB)\")\n",
    "    plt.title(title + f\" (frame={frame_idx})\")\n",
    "    if fmax is not None:\n",
    "        plt.xlim(0, fmax)\n",
    "    if save_as is not None:\n",
    "        out = FIG_DIR / save_as\n",
    "        plt.savefig(out, dpi=160, bbox_inches=\"tight\")\n",
    "        print(\"Saved figure ->\", out)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61572d2a",
   "metadata": {},
   "source": [
    "---\n",
    "## 1) Choose a clip from your manifest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e437480",
   "metadata": {},
   "source": [
    "#### ‚úÖ Checkpoint: choose clips you can explain\n",
    "Pick clips where you can point to regions like:\n",
    "- steady **vowel** (voiced)  \n",
    "- sustained **/s/** (unvoiced)  \n",
    "- **stop burst** (brief)  \n",
    "- **silence/closure** (low energy)\n",
    "\n",
    "**In-class goal:** you should be able to say ‚Äúthis region is voiced/unvoiced/stop/silence‚Äù *before* plotting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b23cc",
   "metadata": {},
   "source": [
    "#### üíæ Save your artifacts\n",
    "Saving plots helps you build a personal ‚Äúspeech atlas‚Äù you can review before exams/projects.\n",
    "If your saved figures folder is empty, check:\n",
    "- you ran the plotting cell\n",
    "- the notebook has write permission in the current folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01764cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_clips()\n",
    "clip_index = 0  # TODO: pick a clip (try vowel, fricative, sentence)\n",
    "\n",
    "m = load_manifest()\n",
    "if len(m[\"audio_clips\"]) > clip_index:\n",
    "    c = m[\"audio_clips\"][clip_index]\n",
    "    fs, y = load_wav(c[\"path\"])\n",
    "    print(\"Using clip:\", c[\"name\"], \"| tags:\", c[\"tags\"])\n",
    "    play_audio(y, fs, label=c[\"name\"])\n",
    "    plot_waveform(y, fs, title=f\"Waveform: {c['name']} (choose interesting times)\", save_as=f\"L7_2_{c['name']}_waveform.png\")\n",
    "else:\n",
    "    raise RuntimeError(\"No such clip index. Record/register clips first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde12b08",
   "metadata": {},
   "source": [
    "---\n",
    "## 2) Compute STFT (manual)\n",
    "Default speech settings:\n",
    "- win=30 ms, hop=10 ms, Hann, nfft=1024\n",
    "\n",
    "**Before running:** predict what voiced vs unvoiced will look like in the spectrogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162aa2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_ms, hop_ms, nfft = 30, 10, 1024\n",
    "f, tt, X, meta = stft_manual(y, fs, win_length_ms=win_ms, hop_ms=hop_ms, window=\"hann\", nfft=nfft, center=True)\n",
    "meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f46b1",
   "metadata": {},
   "source": [
    "#### üíæ Save your artifacts\n",
    "Saving plots helps you build a personal ‚Äúspeech atlas‚Äù you can review before exams/projects.\n",
    "If your saved figures folder is empty, check:\n",
    "- you ran the plotting cell\n",
    "- the notebook has write permission in the current folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram_db(f, tt, X,\n",
    "                    title=f\"Spectrogram: {c['name']} (win={win_ms}ms, hop={hop_ms}ms)\",\n",
    "                    fmax=min(8000, fs/2),\n",
    "                    save_as=f\"L7_2_{c['name']}_spec.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1e6487-91ca-4a18-a572-e24dec26c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram_db(f, tt, X,\n",
    "                    title=f\"Spectrogram: {c['name']} (win={win_ms}ms, hop={hop_ms}ms)\",\n",
    "                    fmax=min(4000, fs/4),\n",
    "                    save_as=f\"L7_2_{c['name']}_spec.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f56e6b8",
   "metadata": {},
   "source": [
    "---\n",
    "## 3) Pick times of interest and inspect frame spectra\n",
    "### How to choose times\n",
    "Look at waveform + spectrogram and choose times for:\n",
    "- voiced (vowel)\n",
    "- unvoiced (fricative)\n",
    "- silence\n",
    "- burst / onset (if present)\n",
    "\n",
    "Then we convert time ‚Üí frame index and plot frame spectra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82f365f",
   "metadata": {},
   "source": [
    "#### üéØ Action: pick *meaningful* time points (not sequential frames)\n",
    "Use the waveform/spectrogram to pick times from **different phenomena**:\n",
    "- vowel middle\n",
    "- fricative middle\n",
    "- stop burst peak\n",
    "- silence/closure\n",
    "\n",
    "**Why?** You want to compare spectra across *sound types*, not across adjacent time slices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05dc525",
   "metadata": {},
   "source": [
    "#### üíæ Save your artifacts\n",
    "Saving plots helps you build a personal ‚Äúspeech atlas‚Äù you can review before exams/projects.\n",
    "If your saved figures folder is empty, check:\n",
    "- you ran the plotting cell\n",
    "- the notebook has write permission in the current folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c74e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: edit these times (seconds) based on what you see\n",
    "times_of_interest_s = [0.5, 1.0, 1.5]  \n",
    "\n",
    "for t0 in times_of_interest_s:\n",
    "    k = frame_index_from_time(t0, fs, hop_ms)\n",
    "    k = max(0, min(k, X.shape[0]-1))\n",
    "    plot_frame_spectrum_from_stft(\n",
    "        f, X, k,\n",
    "        title=f\"{c['name']} | spectrum around t={t0:.2f}s\",\n",
    "        fmax=min(4000, fs/2),\n",
    "        save_as=f\"L7_2_{c['name']}_frameSpec_t{t0:.2f}.png\".replace(\".\",\"p\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9946fd2",
   "metadata": {},
   "source": [
    "### Quick check ‚úÖ\n",
    "- Voiced frame: do you see harmonic peaks?\n",
    "- Unvoiced frame: do you see broadband energy?\n",
    "- Silence: is overall magnitude low?\n",
    "\n",
    "If not:\n",
    "- pick better times (or a better clip)\n",
    "- try different window lengths (Notebook 7.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3fb95",
   "metadata": {},
   "source": [
    "---\n",
    "## 4) Summary + what‚Äôs next\n",
    "Write in sentences:\n",
    "1) What did you learn about voiced/unvoiced from spectrogram + frame spectra?\n",
    "2) What was hard (choosing times? seeing harmonics?)\n",
    "3) What parameter would you tune next and why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa0661",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ End-of-notebook wrap-up \n",
    "**Reflection (write in sentences):**\n",
    "- What parameter change produced the *biggest visible difference* today?\n",
    "- Which plot helped you most: waveform, spectrum, or spectrogram ‚Äî and why?\n",
    "- Name one mistake you made (or could make) and how you would debug it.\n",
    "\n",
    "**What‚Äôs next:** proceed to the next Lecture notebook and reuse the same saved recordings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b0a920-9eda-4582-bc69-0c1010979463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
