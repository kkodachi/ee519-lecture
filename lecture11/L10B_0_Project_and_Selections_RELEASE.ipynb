{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "616e12d2",
   "metadata": {},
   "source": [
    "# Lecture 10B — Notebook 10B.0: Recording Queue, Imports, and Segment Selections\n",
    "\n",
    "**Purpose:** Create the L10B project + manifest workflow. Record/import multiple clips and store reusable named segments for mel/MFCC analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456abf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal as sig\n",
    "import scipy.fft as fft\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional audio playback\n",
    "try:\n",
    "    from IPython.display import Audio, display\n",
    "    HAS_IPY_AUDIO = True\n",
    "except Exception:\n",
    "    HAS_IPY_AUDIO = False\n",
    "\n",
    "# Optional recording\n",
    "try:\n",
    "    import sounddevice as sd\n",
    "    HAS_SD = True\n",
    "except Exception as e:\n",
    "    HAS_SD = False\n",
    "    print(\"sounddevice not available (recording disabled).\", e)\n",
    "\n",
    "# ---------- Project paths ----------\n",
    "PROJECT_ROOT = Path.cwd() / \"EE519_L10B_Project\"\n",
    "REC_DIR = PROJECT_ROOT / \"recordings\"\n",
    "FIG_DIR = PROJECT_ROOT / \"figures\"\n",
    "RES_DIR = PROJECT_ROOT / \"results\"\n",
    "MANIFEST_PATH = PROJECT_ROOT / \"manifest.json\"\n",
    "\n",
    "for d in [REC_DIR, FIG_DIR, RES_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_manifest(path=MANIFEST_PATH):\n",
    "    if path.exists():\n",
    "        return json.loads(path.read_text())\n",
    "    return {\"course\":\"EE519\",\"lecture\":\"10B\",\"created_utc\":None,\"clips\":[]}\n",
    "\n",
    "def save_manifest(manifest, path=MANIFEST_PATH):\n",
    "    if manifest.get(\"created_utc\") is None:\n",
    "        manifest[\"created_utc\"] = str(np.datetime64(\"now\"))\n",
    "    path.write_text(json.dumps(manifest, indent=2))\n",
    "    print(\"Saved manifest:\", path)\n",
    "\n",
    "def save_fig(fig, name, dpi=150):\n",
    "    out = FIG_DIR / name\n",
    "    fig.savefig(out, dpi=dpi, bbox_inches=\"tight\")\n",
    "    print(\"Saved figure:\", out)\n",
    "    return out\n",
    "\n",
    "# ---------- WAV I/O ----------\n",
    "import wave\n",
    "def write_wav(path: Path, x: np.ndarray, fs: int):\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    x = np.clip(x, -1.0, 1.0)\n",
    "    x_i16 = (x * 32767.0).astype(np.int16)\n",
    "    with wave.open(str(path), \"wb\") as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)\n",
    "        wf.setframerate(fs)\n",
    "        wf.writeframes(x_i16.tobytes())\n",
    "\n",
    "def read_wav(path: Path):\n",
    "    with wave.open(str(path), \"rb\") as wf:\n",
    "        fs = wf.getframerate()\n",
    "        n = wf.getnframes()\n",
    "        x = np.frombuffer(wf.readframes(n), dtype=np.int16).astype(np.float32) / 32768.0\n",
    "    return fs, x\n",
    "\n",
    "def peak_normalize(x, target=0.98):\n",
    "    m = np.max(np.abs(x)) + 1e-12\n",
    "    return (x / m) * target\n",
    "\n",
    "def play_audio(x, fs, label=\"audio\"):\n",
    "    if not HAS_IPY_AUDIO:\n",
    "        print(\"(Audio playback not available)\", label)\n",
    "        return\n",
    "    display(Audio(x, rate=fs))\n",
    "\n",
    "def record_clip(seconds=2.0, fs=16000):\n",
    "    if not HAS_SD:\n",
    "        raise RuntimeError(\"sounddevice not available. Load wav files instead.\")\n",
    "    print(f\"Recording {seconds:.1f}s @ {fs} Hz ...\")\n",
    "    x = sd.rec(int(seconds*fs), samplerate=fs, channels=1, dtype=\"float32\")\n",
    "    sd.wait()\n",
    "    return fs, x.squeeze()\n",
    "\n",
    "def add_clip_to_manifest(filename, label, fs, notes=\"\"):\n",
    "    clip = {\n",
    "        \"filename\": filename,\n",
    "        \"label\": label,\n",
    "        \"fs\": int(fs),\n",
    "        \"notes\": notes,\n",
    "        \"added_utc\": str(np.datetime64(\"now\")),\n",
    "        \"selections\": {}\n",
    "    }\n",
    "    manifest[\"clips\"].append(clip)\n",
    "    save_manifest(manifest)\n",
    "    return len(manifest[\"clips\"]) - 1\n",
    "\n",
    "def list_clips():\n",
    "    for i,c in enumerate(manifest[\"clips\"]):\n",
    "        print(f\"[{i}] {c['label']:14s}  {c['filename']}  fs={c['fs']}  notes={c.get('notes','')}\")\n",
    "\n",
    "# ---------- Selection + framing helpers ----------\n",
    "def seconds_to_samples(t0, t1, fs, xlen):\n",
    "    s0 = int(max(0, round(t0*fs)))\n",
    "    s1 = int(min(xlen, round(t1*fs)))\n",
    "    if s1 <= s0:\n",
    "        raise ValueError(\"Bad selection: t1 must be > t0\")\n",
    "    return s0, s1\n",
    "\n",
    "def samples_to_frame_range(s0, s1, N, H, xlen):\n",
    "    f0 = max(0, int((s0 - N)//H) + 1)\n",
    "    f1 = min(int((s1)//H), int((xlen-N)//H))\n",
    "    if f1 < f0:\n",
    "        f0 = max(0, int(s0//H))\n",
    "        f1 = min(int((xlen-N)//H), f0)\n",
    "    return f0, f1\n",
    "\n",
    "def frame_signal(x, N, H):\n",
    "    if len(x) < N:\n",
    "        x = np.pad(x, (0, N-len(x)))\n",
    "    num = 1 + (len(x) - N)//H\n",
    "    idx = np.arange(N)[None,:] + H*np.arange(num)[:,None]\n",
    "    return x[idx]\n",
    "\n",
    "def db(x):\n",
    "    return 20*np.log10(np.maximum(x, 1e-12))\n",
    "\n",
    "# ---------- Auditory scale helpers ----------\n",
    "def hz_to_mel(hz):\n",
    "    return 2595.0 * np.log10(1.0 + hz/700.0)\n",
    "\n",
    "def mel_to_hz(mel):\n",
    "    return 700.0 * (10**(mel/2595.0) - 1.0)\n",
    "\n",
    "def mel_filterbank(fs, nfft, n_mels=26, fmin=0.0, fmax=None):\n",
    "    if fmax is None:\n",
    "        fmax = fs/2\n",
    "    # mel points\n",
    "    mmin, mmax = hz_to_mel(fmin), hz_to_mel(fmax)\n",
    "    m_pts = np.linspace(mmin, mmax, n_mels+2)\n",
    "    hz_pts = mel_to_hz(m_pts)\n",
    "    # fft bin frequencies\n",
    "    freqs = np.linspace(0, fs/2, nfft//2 + 1)\n",
    "    bins = np.floor((nfft+1) * hz_pts / fs).astype(int)\n",
    "    fb = np.zeros((n_mels, len(freqs)), dtype=np.float64)\n",
    "    for i in range(n_mels):\n",
    "        b0, b1, b2 = bins[i], bins[i+1], bins[i+2]\n",
    "        b0 = np.clip(b0, 0, len(freqs)-1)\n",
    "        b1 = np.clip(b1, 0, len(freqs)-1)\n",
    "        b2 = np.clip(b2, 0, len(freqs)-1)\n",
    "        if b1 == b0: b1 += 1\n",
    "        if b2 == b1: b2 += 1\n",
    "        # rising\n",
    "        fb[i, b0:b1] = (np.arange(b0, b1) - b0) / (b1 - b0 + 1e-12)\n",
    "        # falling\n",
    "        fb[i, b1:b2] = (b2 - np.arange(b1, b2)) / (b2 - b1 + 1e-12)\n",
    "    return fb, freqs, hz_pts\n",
    "\n",
    "manifest = load_manifest()\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Clips in manifest:\", len(manifest[\"clips\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30a2be",
   "metadata": {},
   "source": [
    "## What you will do\n",
    "- Record (or import) multiple audio clips\n",
    "- Store them in `EE519_L10B_Project/recordings/`\n",
    "- Create **named selections** (time ranges) per clip inside `manifest.json`\n",
    "- Save a few “quick-look” plots for sanity checks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf2dd7",
   "metadata": {},
   "source": [
    "## 1) Recording queue (recommended)\n",
    "\n",
    "Targets (suggested):\n",
    "- `vowel_a` (steady /a/)\n",
    "- `vowel_i` (steady /i/)\n",
    "- `fricative_s` (steady /s/)\n",
    "- `sentence` (short sentence)\n",
    "\n",
    "If recording isn't available, skip to **Import WAVs**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_RECORD = True  # set True in class\n",
    "\n",
    "TARGETS = [\n",
    "    (\"vowel_a\", 2.0, \"steady /a/\"),\n",
    "    (\"vowel_i\", 2.0, \"steady /i/\"),\n",
    "    (\"fricative_s\", 2.0, \"steady /s/\"),\n",
    "    (\"sentence\", 3.0, \"short sentence\"),\n",
    "]\n",
    "\n",
    "if DO_RECORD:\n",
    "    for label, seconds, notes in TARGETS:\n",
    "        fs, x = record_clip(seconds=seconds, fs=16000)\n",
    "        x = peak_normalize(x)\n",
    "        fname = f\"student10B_{label}.wav\"\n",
    "        write_wav(REC_DIR/fname, x, fs)\n",
    "        add_clip_to_manifest(fname, label=label, fs=fs, notes=notes)\n",
    "        print(\"Recorded:\", fname)\n",
    "        play_audio(x, fs, label)\n",
    "else:\n",
    "    print(\"Skipping recording (set DO_RECORD=True).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106488dd",
   "metadata": {},
   "source": [
    "## 2) Import WAV files (fallback)\n",
    "\n",
    "Drop WAV files into:\n",
    "`EE519_L10B_Project/recordings/`\n",
    "and run this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9843db",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing = set([c[\"filename\"] for c in manifest[\"clips\"]])\n",
    "wav_files = sorted([p.name for p in REC_DIR.glob(\"*.wav\")])\n",
    "added = 0\n",
    "for wf in wav_files:\n",
    "    if wf not in existing:\n",
    "        fs, _ = read_wav(REC_DIR/wf)\n",
    "        add_clip_to_manifest(wf, label=\"imported\", fs=fs, notes=\"auto-import\")\n",
    "        added += 1\n",
    "\n",
    "print(\"WAV files:\", wav_files)\n",
    "print(\"Newly added:\", added)\n",
    "print(\"Total clips:\", len(manifest[\"clips\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0cee7",
   "metadata": {},
   "source": [
    "## 3) Create named selections (time ranges)\n",
    "\n",
    "We store selections at:\n",
    "`clip[\"selections\"][\"analysis_segments\"][segment_name]`\n",
    "\n",
    "Recommended segment names:\n",
    "- `vowel_mid`\n",
    "- `fricative_mid`\n",
    "- `sentence_voiced`\n",
    "- `sentence_unvoiced` (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_clips()\n",
    "CLIP_IDX = 3\n",
    "clip = manifest[\"clips\"][CLIP_IDX]\n",
    "fs, x = read_wav(REC_DIR / clip[\"filename\"])\n",
    "x = peak_normalize(x)\n",
    "\n",
    "print(\"Selected:\", CLIP_IDX, clip[\"label\"], clip[\"filename\"], \"fs=\", fs, \"len_sec=\", len(x)/fs)\n",
    "play_audio(x, fs, clip[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05edc63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment parameters (stored with selection)\n",
    "WIN_MS, HOP_MS = 25, 10\n",
    "N = int(WIN_MS*1e-3*fs)\n",
    "H = int(HOP_MS*1e-3*fs)\n",
    "\n",
    "segment_name = \"seg1\"  # change to vowel_mid, fricative_mid, etc.\n",
    "t0, t1 = 0.50, 1.20    # seconds\n",
    "\n",
    "s0, s1 = seconds_to_samples(t0, t1, fs, len(x))\n",
    "f0, f1 = samples_to_frame_range(s0, s1, N, H, len(x))\n",
    "\n",
    "clip = manifest[\"clips\"][CLIP_IDX]\n",
    "clip.setdefault(\"selections\", {}).setdefault(\"analysis_segments\", {})\n",
    "clip[\"selections\"][\"analysis_segments\"][segment_name] = {\n",
    "    \"t0\": float(t0), \"t1\": float(t1),\n",
    "    \"s0\": int(s0), \"s1\": int(s1),\n",
    "    \"win_ms\": float(WIN_MS), \"hop_ms\": float(HOP_MS),\n",
    "    \"N\": int(N), \"H\": int(H),\n",
    "    \"frame_range\": [int(f0), int(f1)]\n",
    "}\n",
    "manifest[\"clips\"][CLIP_IDX] = clip\n",
    "save_manifest(manifest)\n",
    "\n",
    "print(\"Stored selection:\", segment_name, \"frames\", (f0, f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0321d898",
   "metadata": {},
   "source": [
    "### Quick-look waveform + selection (save to figures/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8698631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(len(x))/fs\n",
    "fig = plt.figure(figsize=(10,3))\n",
    "plt.plot(t, x, linewidth=0.8)\n",
    "plt.axvspan(t0, t1, alpha=0.2)\n",
    "plt.title(f\"Waveform + selection '{segment_name}': clip {CLIP_IDX} ({clip['label']})\")\n",
    "plt.xlabel(\"Time (s)\"); plt.ylabel(\"Amplitude\")\n",
    "plt.tight_layout(); plt.show()\n",
    "save_fig(fig, f\"L10B_sel_clip{CLIP_IDX}_{re.sub(r'[^a-zA-Z0-9_]+','_',segment_name)}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13e176a",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "**What you learned:** project+manifest workflow for L10B.  \n",
    "**What’s next:** 10B.1 builds mel scale + filterbanks and shows how they tile frequency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5d6b77",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "1) Which segments are most stationary (good for short-time analysis)? Why?  \n",
    "2) Why do we store selections in the manifest instead of hardcoding time indices?  \n",
    "3) What naming scheme helps you reuse selections across notebooks?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49066d-e4a0-4c5d-85a2-3804968ba6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
