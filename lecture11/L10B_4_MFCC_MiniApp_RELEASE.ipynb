{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bd00df",
   "metadata": {},
   "source": [
    "# Lecture 10B — Notebook 10B.4: Mini-App — Simple Classification with MFCC Features\n",
    "\n",
    "**Purpose:** Use MFCC features from your recorded clips to build a tiny end-to-end ML demo (vowel vs fricative, or speaker style).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f4ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal as sig\n",
    "import scipy.fft as fft\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional audio playback\n",
    "try:\n",
    "    from IPython.display import Audio, display\n",
    "    HAS_IPY_AUDIO = True\n",
    "except Exception:\n",
    "    HAS_IPY_AUDIO = False\n",
    "\n",
    "# Optional recording (works only if your environment supports it)\n",
    "try:\n",
    "    import sounddevice as sd\n",
    "    HAS_SD = True\n",
    "except Exception as e:\n",
    "    HAS_SD = False\n",
    "    print(\"sounddevice not available (recording disabled).\", e)\n",
    "\n",
    "# ============================================================\n",
    "# 0) Robust project/manifest discovery\n",
    "# ============================================================\n",
    "# We try a few common locations so the notebook runs:\n",
    "# - inside a shared folder (manifest.json next to the notebook)\n",
    "# - inside the provided course folder EE519_L10B_Project\n",
    "# - in this sandbox environment (/mnt/data/manifest.json)\n",
    "CANDIDATE_MANIFESTS = [\n",
    "    Path(\"manifest.json\"),\n",
    "    Path.cwd() / \"manifest.json\",\n",
    "    Path.cwd() / \"EE519_L10B_Project\" / \"manifest.json\",\n",
    "    Path(\"/mnt/data/manifest.json\"),\n",
    "]\n",
    "\n",
    "def first_existing(paths):\n",
    "    for p in paths:\n",
    "        try:\n",
    "            if p.exists():\n",
    "                return p\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "MANIFEST_PATH = first_existing(CANDIDATE_MANIFESTS)\n",
    "\n",
    "# If nothing exists yet, we create a default course folder structure\n",
    "if MANIFEST_PATH is None:\n",
    "    PROJECT_ROOT = Path.cwd() / \"EE519_L10B_Project\"\n",
    "    PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    MANIFEST_PATH = PROJECT_ROOT / \"manifest.json\"\n",
    "else:\n",
    "    PROJECT_ROOT = MANIFEST_PATH.parent\n",
    "\n",
    "# Default subfolders (created if you want to record/organize locally)\n",
    "REC_DIR = PROJECT_ROOT / \"recordings\"\n",
    "FIG_DIR = PROJECT_ROOT / \"figures\"\n",
    "RES_DIR = PROJECT_ROOT / \"results\"\n",
    "for d in [REC_DIR, FIG_DIR, RES_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Using PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"Using MANIFEST_PATH:\", MANIFEST_PATH)\n",
    "\n",
    "# ============================================================\n",
    "# 1) Manifest helpers\n",
    "# ============================================================\n",
    "def load_manifest(path=MANIFEST_PATH):\n",
    "    if path.exists():\n",
    "        return json.loads(path.read_text())\n",
    "    return {\"course\":\"EE519\",\"lecture\":\"10B\",\"created_utc\":None,\"clips\":[]}\n",
    "\n",
    "def save_manifest(manifest, path=MANIFEST_PATH):\n",
    "    if manifest.get(\"created_utc\") is None:\n",
    "        manifest[\"created_utc\"] = str(np.datetime64(\"now\"))\n",
    "    path.write_text(json.dumps(manifest, indent=2))\n",
    "    print(\"Saved manifest:\", path)\n",
    "\n",
    "def save_fig(fig, name, dpi=150):\n",
    "    out = FIG_DIR / name\n",
    "    fig.savefig(out, dpi=dpi, bbox_inches=\"tight\")\n",
    "    print(\"Saved figure:\", out)\n",
    "    return out\n",
    "\n",
    "manifest = load_manifest()\n",
    "print(\"Clips in manifest:\", len(manifest.get(\"clips\", [])))\n",
    "\n",
    "def list_clips():\n",
    "    clips = manifest.get(\"clips\", [])\n",
    "    if len(clips) == 0:\n",
    "        print(\"(No clips in manifest yet)\")\n",
    "        return\n",
    "    for i,c in enumerate(clips):\n",
    "        print(f\"[{i}] {c.get('label','?'):14s}  {c.get('filename','?')}  fs={c.get('fs','?')}  notes={c.get('notes','')}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) WAV I/O (pure-Python, robust)\n",
    "# ============================================================\n",
    "import wave\n",
    "\n",
    "def write_wav(path: Path, x: np.ndarray, fs: int):\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    x = np.clip(x, -1.0, 1.0)\n",
    "    x_i16 = (x * 32767.0).astype(np.int16)\n",
    "    with wave.open(str(path), \"wb\") as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)\n",
    "        wf.setframerate(int(fs))\n",
    "        wf.writeframes(x_i16.tobytes())\n",
    "\n",
    "def read_wav(path: Path):\n",
    "    \"\"\"Read 16-bit PCM mono WAV -> float32 in [-1, 1].\"\"\"\n",
    "    with wave.open(str(path), \"rb\") as wf:\n",
    "        fs = wf.getframerate()\n",
    "        n = wf.getnframes()\n",
    "        chans = wf.getnchannels()\n",
    "        sampwidth = wf.getsampwidth()\n",
    "        if chans != 1 or sampwidth != 2:\n",
    "            raise ValueError(f\"Expected mono 16-bit PCM WAV. Got channels={chans}, sampwidth={sampwidth} bytes\")\n",
    "        x = np.frombuffer(wf.readframes(n), dtype=np.int16).astype(np.float32) / 32768.0\n",
    "    return int(fs), x\n",
    "\n",
    "def peak_normalize(x, target=0.98):\n",
    "    m = float(np.max(np.abs(x)) + 1e-12)\n",
    "    return (x / m) * target\n",
    "\n",
    "def play_audio(x, fs, label=\"audio\"):\n",
    "    if not HAS_IPY_AUDIO:\n",
    "        print(\"(Audio playback not available)\", label)\n",
    "        return\n",
    "    display(Audio(x, rate=fs))\n",
    "\n",
    "def record_clip(seconds=2.0, fs=16000):\n",
    "    if not HAS_SD:\n",
    "        raise RuntimeError(\"sounddevice not available. Load wav files instead.\")\n",
    "    print(f\"Recording {seconds:.1f}s @ {fs} Hz ...\")\n",
    "    x = sd.rec(int(seconds*fs), samplerate=fs, channels=1, dtype=\"float32\")\n",
    "    sd.wait()\n",
    "    return int(fs), x.squeeze()\n",
    "\n",
    "def add_clip_to_manifest(filename, label, fs, notes=\"\"):\n",
    "    clip = {\n",
    "        \"filename\": filename,\n",
    "        \"label\": label,\n",
    "        \"fs\": int(fs),\n",
    "        \"notes\": notes,\n",
    "        \"added_utc\": str(np.datetime64(\"now\")),\n",
    "        \"selections\": {}\n",
    "    }\n",
    "    manifest.setdefault(\"clips\", []).append(clip)\n",
    "    save_manifest(manifest)\n",
    "    return len(manifest[\"clips\"]) - 1\n",
    "\n",
    "# ============================================================\n",
    "# 3) Audio path resolver (important for sharing notebooks)\n",
    "# ============================================================\n",
    "def resolve_audio_path(filename: str) -> Path:\n",
    "    \"\"\"Try common locations for audio files referenced by the manifest.\"\"\"\n",
    "    p = Path(filename)\n",
    "    if p.is_absolute() and p.exists():\n",
    "        return p\n",
    "\n",
    "    candidates = [\n",
    "        PROJECT_ROOT / filename,\n",
    "        PROJECT_ROOT / \"recordings\" / filename,\n",
    "        REC_DIR / filename,\n",
    "        Path.cwd() / filename,\n",
    "        Path.cwd() / \"recordings\" / filename,\n",
    "        Path(\"/mnt/data\") / filename,\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            return c\n",
    "    # If nothing found, return the most likely path (helps debug)\n",
    "    return PROJECT_ROOT / filename\n",
    "\n",
    "# ---------- Selection + framing helpers ----------\n",
    "def seconds_to_samples(t0, t1, fs, xlen):\n",
    "    s0 = int(max(0, round(float(t0)*fs)))\n",
    "    s1 = int(min(int(xlen), round(float(t1)*fs)))\n",
    "    if s1 <= s0:\n",
    "        raise ValueError(\"Bad selection: t1 must be > t0\")\n",
    "    return s0, s1\n",
    "\n",
    "def samples_to_frame_range(s0, s1, N, H, xlen):\n",
    "    f0 = max(0, int((s0 - N)//H) + 1)\n",
    "    f1 = min(int((s1)//H), int((xlen-N)//H))\n",
    "    if f1 < f0:\n",
    "        f0 = max(0, int(s0//H))\n",
    "        f1 = min(int((xlen-N)//H), f0)\n",
    "    return f0, f1\n",
    "\n",
    "def frame_signal(x, N, H):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    if len(x) < N:\n",
    "        x = np.pad(x, (0, N-len(x)))\n",
    "    num = 1 + (len(x) - N)//H\n",
    "    idx = np.arange(N)[None,:] + H*np.arange(num)[:,None]\n",
    "    return x[idx]\n",
    "\n",
    "def db(x):\n",
    "    return 20*np.log10(np.maximum(x, 1e-12))\n",
    "\n",
    "# ---------- Auditory scale helpers ----------\n",
    "def hz_to_mel(hz):\n",
    "    return 2595.0 * np.log10(1.0 + hz/700.0)\n",
    "\n",
    "def mel_to_hz(mel):\n",
    "    return 700.0 * (10**(mel/2595.0) - 1.0)\n",
    "\n",
    "def mel_filterbank(fs, nfft, n_mels=26, fmin=0.0, fmax=None):\n",
    "    if fmax is None:\n",
    "        fmax = fs/2\n",
    "    mmin, mmax = hz_to_mel(fmin), hz_to_mel(fmax)\n",
    "    m_pts = np.linspace(mmin, mmax, n_mels+2)\n",
    "    hz_pts = mel_to_hz(m_pts)\n",
    "\n",
    "    freqs = np.linspace(0, fs/2, nfft//2 + 1)\n",
    "    bins = np.floor((nfft+1) * hz_pts / fs).astype(int)\n",
    "\n",
    "    fb = np.zeros((n_mels, len(freqs)), dtype=np.float64)\n",
    "    for i in range(n_mels):\n",
    "        b0, b1, b2 = bins[i], bins[i+1], bins[i+2]\n",
    "        b0 = np.clip(b0, 0, len(freqs)-1)\n",
    "        b1 = np.clip(b1, 0, len(freqs)-1)\n",
    "        b2 = np.clip(b2, 0, len(freqs)-1)\n",
    "        if b1 == b0: b1 = min(b0+1, len(freqs)-1)\n",
    "        if b2 == b1: b2 = min(b1+1, len(freqs)-1)\n",
    "\n",
    "        fb[i, b0:b1] = (np.arange(b0, b1) - b0) / (b1 - b0 + 1e-12)\n",
    "        fb[i, b1:b2] = (b2 - np.arange(b1, b2)) / (b2 - b1 + 1e-12)\n",
    "    return fb, freqs, hz_pts\n",
    "\n",
    "# quick peek\n",
    "list_clips()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238c9d0",
   "metadata": {},
   "source": [
    "## 0) What this mini-app does\n",
    "\n",
    "We will:\n",
    "1) choose two (or three) clips (e.g., vowel vs fricative)\n",
    "2) compute MFCC features per frame\n",
    "3) aggregate statistics per clip (mean + std)\n",
    "4) train a simple classifier (logistic regression) and evaluate\n",
    "\n",
    "**Note:** This is a teaching “toy” pipeline: it’s about feature intuition, not SOTA accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1078e64",
   "metadata": {},
   "source": [
    "## 1) Load clips + assign class labels\n",
    "\n",
    "This notebook supports two common workflows:\n",
    "\n",
    "1) **Use an existing `manifest.json` + wav files** (recommended for grading / sharing).  \n",
    "2) **Record new clips locally** (if `sounddevice` is available).\n",
    "\n",
    "### How class labels work (for ML)\n",
    "We will build a dataset where **each clip becomes one training example**:\n",
    "- Input features: MFCC summary statistics over your selected segment (mean + std).\n",
    "- Target label: derived from the clip's `label` field (e.g., `vowel_a`, `vowel_i`, `fricative_s`, `sentence`).\n",
    "\n",
    "You can:\n",
    "- **Use automatic mapping** from the `label` strings (default), or  \n",
    "- **Override** with a custom `LABEL_MAP` / `KEEP_LABELS` list below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffeaa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "list_clips()\n",
    "\n",
    "# ============================================================\n",
    "# Class-label control\n",
    "# ============================================================\n",
    "# The manifest clip field `label` is a string, e.g.:\n",
    "#   \"vowel_a\", \"vowel_i\", \"fricative_s\", \"sentence\"\n",
    "#\n",
    "# Option A (default): use labels as-is (maybe filtered by KEEP_LABELS).\n",
    "# Option B: map multiple raw labels into a smaller set of classes via LABEL_MAP.\n",
    "\n",
    "# Keep only these labels (set to None to keep everything)\n",
    "KEEP_LABELS = None\n",
    "# Example:\n",
    "# KEEP_LABELS = [\"vowel_a\", \"vowel_i\", \"fricative_s\"]\n",
    "\n",
    "# Map labels -> merged classes (set to None to disable)\n",
    "LABEL_MAP = None\n",
    "# Example (binary vowel vs fricative):\n",
    "# LABEL_MAP = {\n",
    "#     \"vowel_a\": \"vowel\",\n",
    "#     \"vowel_i\": \"vowel\",\n",
    "#     \"fricative_s\": \"fricative\",\n",
    "# }\n",
    "\n",
    "def clip_to_class(label: str) -> str:\n",
    "    lab = str(label).strip()\n",
    "    if LABEL_MAP is not None:\n",
    "        return LABEL_MAP.get(lab, None)\n",
    "    return lab\n",
    "\n",
    "def should_keep(label: str) -> bool:\n",
    "    if KEEP_LABELS is None:\n",
    "        return True\n",
    "    return str(label).strip() in set(KEEP_LABELS)\n",
    "\n",
    "print(\"KEEP_LABELS:\", KEEP_LABELS)\n",
    "print(\"LABEL_MAP:\", LABEL_MAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482992ad",
   "metadata": {},
   "source": [
    "## 2) MFCC feature extractor (reusing earlier steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a49b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dct2_matrix(N, K):\n",
    "    n = np.arange(N)[None,:]\n",
    "    k = np.arange(K)[:,None]\n",
    "    D = np.cos(np.pi/N * (n + 0.5) * k)\n",
    "    D[0,:] *= 1/np.sqrt(N)\n",
    "    D[1:,:] *= np.sqrt(2/N)\n",
    "    return D\n",
    "\n",
    "def compute_mfcc_for_segment(fs, xseg, win_ms=25, hop_ms=10, n_mels=40, n_ceps=13):\n",
    "    N = int(win_ms*1e-3*fs)\n",
    "    H = int(hop_ms*1e-3*fs)\n",
    "    window = sig.windows.hann(N, sym=False)\n",
    "    frames = frame_signal(xseg, N, H)\n",
    "    NFFT = 2048 if fs <= 16000 else 4096\n",
    "    X = fft.rfft(frames * window[None,:], n=NFFT, axis=1)\n",
    "    P = (np.abs(X)**2) / NFFT\n",
    "    fb, _, _ = mel_filterbank(fs, NFFT, n_mels=n_mels)\n",
    "    logM = np.log(P @ fb.T + 1e-12)\n",
    "    D = dct2_matrix(n_mels, n_ceps)\n",
    "    mfcc = logM @ D.T\n",
    "    return mfcc  # (T, n_ceps)\n",
    "\n",
    "def aggregate_stats(feat):\n",
    "    # simple clip-level representation: mean and std of each coefficient\n",
    "    mu = np.mean(feat, axis=0)\n",
    "    sd = np.std(feat, axis=0)\n",
    "    return np.concatenate([mu, sd], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ddf9d",
   "metadata": {},
   "source": [
    "## 3) Build dataset (X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f21e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3) Build dataset (choose your granularity)\n",
    "# ============================================================\n",
    "# Why your dataset can look \"very small\":\n",
    "# - In the simplest setting we create **one feature vector per clip**\n",
    "#   by summarizing MFCCs across time (mean+std). That yields only as\n",
    "#   many samples as you have recorded clips (often just a few).\n",
    "#\n",
    "# But MFCC analysis produces many *frames* per clip. To leverage that,\n",
    "# we can create **multiple training examples per clip** by chunking a\n",
    "# segment into overlapping windows (recommended), or by using each MFCC\n",
    "# frame as an example (very large, can overfit, more care needed).\n",
    "#\n",
    "# Choose one of:\n",
    "#   DATASET_MODE = \"clip\"    -> 1 example per clip (smallest)\n",
    "#   DATASET_MODE = \"window\"  -> many examples per clip (best demo)\n",
    "#   DATASET_MODE = \"frame\"   -> 1 example per MFCC frame (largest)\n",
    "DATASET_MODE = \"window\"\n",
    "\n",
    "# Windowing settings (only used when DATASET_MODE == \"window\")\n",
    "WINDOW_SEC = 0.25       # seconds per window\n",
    "WINDOW_HOP_SEC = 0.05   # seconds hop between windows\n",
    "\n",
    "# Use all saved analysis segments (seg1, seg2, ...) instead of only seg1\n",
    "USE_ALL_SEGMENTS = True\n",
    "\n",
    "# If a clip has no saved selections, fall back to using the full clip\n",
    "FALLBACK_TO_FULL_CLIP = True\n",
    "\n",
    "# Minimum number of MFCC frames required to keep an example\n",
    "MIN_MFCC_FRAMES = 3\n",
    "\n",
    "def aggregate_stats(feat_TxD):\n",
    "    \"\"\"feat_TxD -> vector of length 2D (mean+std).\"\"\"\n",
    "    mu = np.mean(feat_TxD, axis=0)\n",
    "    sd = np.std(feat_TxD, axis=0)\n",
    "    return np.concatenate([mu, sd], axis=0)\n",
    "\n",
    "X_list, y_list, name_list = [], [], []\n",
    "\n",
    "clips = manifest.get(\"clips\", [])\n",
    "for i, clip in enumerate(clips):\n",
    "    raw_label = clip.get(\"label\", \"\")\n",
    "    if not should_keep(raw_label):\n",
    "        continue\n",
    "\n",
    "    yname = clip_to_class(raw_label)\n",
    "    if yname is None:\n",
    "        continue\n",
    "\n",
    "    wav_path = resolve_audio_path(clip.get(\"filename\", \"\"))\n",
    "    if not wav_path.exists():\n",
    "        print(f\"⚠️ Missing wav for clip {i}: {wav_path}\")\n",
    "        continue\n",
    "\n",
    "    fs, x = read_wav(wav_path)\n",
    "    x = peak_normalize(x)\n",
    "\n",
    "    segs = (clip.get(\"selections\", {}) or {}).get(\"analysis_segments\", {}) or {}\n",
    "\n",
    "    # If no saved selections exist, optionally use the full clip\n",
    "    if len(segs) == 0 and FALLBACK_TO_FULL_CLIP:\n",
    "        segs = {\n",
    "            \"full\": {\"s0\": 0, \"s1\": len(x), \"win_ms\": 25.0, \"hop_ms\": 10.0, \"n_mels\": 40, \"n_ceps\": 13}\n",
    "        }\n",
    "\n",
    "    if len(segs) == 0:\n",
    "        # nothing to train on\n",
    "        continue\n",
    "\n",
    "    # Decide which segment(s) to use\n",
    "    seg_keys = list(segs.keys())\n",
    "    if not USE_ALL_SEGMENTS:\n",
    "        seg_keys = [\"seg1\"] if \"seg1\" in segs else [seg_keys[0]]\n",
    "\n",
    "    for seg_name in seg_keys:\n",
    "        sel = segs[seg_name]\n",
    "\n",
    "        s0 = int(sel.get(\"s0\", 0))\n",
    "        s1 = int(sel.get(\"s1\", len(x)))\n",
    "        s0 = max(0, min(s0, len(x)-1))\n",
    "        s1 = max(s0+1, min(s1, len(x)))\n",
    "        xseg = x[s0:s1]\n",
    "\n",
    "        win_ms = float(sel.get(\"win_ms\", 25.0))\n",
    "        hop_ms = float(sel.get(\"hop_ms\", 10.0))\n",
    "        n_mels = int(sel.get(\"n_mels\", 40))\n",
    "        n_ceps = int(sel.get(\"n_ceps\", 13))\n",
    "\n",
    "        mfcc = compute_mfcc_for_segment(\n",
    "            fs, xseg,\n",
    "            win_ms=win_ms, hop_ms=hop_ms,\n",
    "            n_mels=n_mels, n_ceps=n_ceps\n",
    "        )  # (T, n_ceps)\n",
    "\n",
    "        T = mfcc.shape[0]\n",
    "        if T < MIN_MFCC_FRAMES:\n",
    "            continue\n",
    "\n",
    "        if DATASET_MODE == \"clip\":\n",
    "            X_list.append(aggregate_stats(mfcc))\n",
    "            y_list.append(yname)\n",
    "            name_list.append(f\"{clip.get('name','clip'+str(i))}:{seg_name}\")\n",
    "\n",
    "        elif DATASET_MODE == \"frame\":\n",
    "            # Each MFCC frame is one sample (D = n_ceps)\n",
    "            # To keep the classifier comparable, we still output mean+std style vector:\n",
    "            # here we just use the frame itself and zeros for std (or you can use deltas).\n",
    "            for t in range(T):\n",
    "                f = mfcc[t:t+1, :]  # 1 x D\n",
    "                vec = np.concatenate([mfcc[t, :], np.zeros(n_ceps)], axis=0)\n",
    "                X_list.append(vec)\n",
    "                y_list.append(yname)\n",
    "                name_list.append(f\"{clip.get('name','clip'+str(i))}:{seg_name}:t{t}\")\n",
    "\n",
    "        elif DATASET_MODE == \"window\":\n",
    "            # Make overlapping windows in MFCC-frame units\n",
    "            frames_per_window = max(1, int(round(WINDOW_SEC / (hop_ms * 1e-3))))\n",
    "            hop_frames = max(1, int(round(WINDOW_HOP_SEC / (hop_ms * 1e-3))))\n",
    "\n",
    "            if T < frames_per_window:\n",
    "                # If segment shorter than window, just keep one example\n",
    "                X_list.append(aggregate_stats(mfcc))\n",
    "                y_list.append(yname)\n",
    "                name_list.append(f\"{clip.get('name','clip'+str(i))}:{seg_name}:short\")\n",
    "            else:\n",
    "                for t0 in range(0, T - frames_per_window + 1, hop_frames):\n",
    "                    block = mfcc[t0:t0 + frames_per_window, :]\n",
    "                    if block.shape[0] < MIN_MFCC_FRAMES:\n",
    "                        continue\n",
    "                    X_list.append(aggregate_stats(block))\n",
    "                    y_list.append(yname)\n",
    "                    name_list.append(f\"{clip.get('name','clip'+str(i))}:{seg_name}:w{t0}\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown DATASET_MODE: \" + str(DATASET_MODE))\n",
    "\n",
    "X = np.array(X_list, dtype=np.float64)\n",
    "y = np.array(y_list)\n",
    "\n",
    "print(f\"Built dataset with DATASET_MODE='{DATASET_MODE}':\")\n",
    "print(\"  X shape:\", X.shape, \"(N_samples, N_features)\")\n",
    "print(\"  y shape:\", y.shape)\n",
    "\n",
    "# Show per-class counts\n",
    "if len(y) > 0:\n",
    "    labs, cnt = np.unique(y, return_counts=True)\n",
    "    print(\"\\nSamples per class:\")\n",
    "    for lab, c in zip(labs, cnt):\n",
    "        print(f\"  {lab:>15s}: {c}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e6d78b",
   "metadata": {},
   "source": [
    "## 4) Train + evaluate (toy demo)\n",
    "\n",
    "If you have only 2–4 samples, evaluation is unstable.  \n",
    "**Tip:** record multiple vowels/fricatives (fast/slow/loud/soft) to create more samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae26237",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(y) < 6:\n",
    "    print(\"⚠️ Very small dataset. Record more clips for a meaningful ML demo.\")\n",
    "\n",
    "if len(np.unique(y)) < 2:\n",
    "    raise ValueError(\"Need at least 2 classes to train a classifier. Check KEEP_LABELS/LABEL_MAP and your manifest labels.\")\n",
    "\n",
    "# Stratify only if every class has >= 2 samples\n",
    "uniq, cnt = np.unique(y, return_counts=True)\n",
    "can_stratify = np.all(cnt >= 2)\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.5,\n",
    "    random_state=0,\n",
    "    stratify=y if can_stratify else None\n",
    ")\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(max_iter=500, multi_class=\"auto\"))\n",
    "])\n",
    "\n",
    "clf.fit(Xtr, ytr)\n",
    "pred = clf.predict(Xte)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(yte, pred))\n",
    "print()\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(yte, pred))\n",
    "\n",
    "cm = confusion_matrix(yte, pred, labels=np.unique(y))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "print(cm)\n",
    "\n",
    "# Simple confusion matrix plot\n",
    "fig, ax = plt.subplots(figsize=(4.5, 4.0))\n",
    "im = ax.imshow(cm, aspect=\"auto\")\n",
    "ax.set_title(\"Confusion Matrix\")\n",
    "labs = np.unique(y)\n",
    "ax.set_xticks(range(len(labs))); ax.set_xticklabels(labs, rotation=45, ha=\"right\")\n",
    "ax.set_yticks(range(len(labs))); ax.set_yticklabels(labs)\n",
    "ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "\n",
    "for r in range(cm.shape[0]):\n",
    "    for c in range(cm.shape[1]):\n",
    "        ax.text(c, r, str(cm[r, c]), ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(fig, \"mfcc_confusion_matrix.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca235db5",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "**What you learned:** MFCC features can feed directly into a simple classifier; clip-level mean/std captures coarse “spectral shape” differences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b4237f",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "1) Which MFCC statistics (mean vs std) do you think separate vowel vs fricative better? Why?  \n",
    "2) What would you change to make this a better experiment (more clips, cross-validation, per-frame classification, etc.)?  \n",
    "3) How would noise affect MFCC features, and what could you do about it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038ee6c-edc4-4d37-97f5-433501cc4135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
